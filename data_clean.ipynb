{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes supprimées de : 2017-05-12_3_6C-80per_3_6C_CH1.csv\n",
      "Colonnes supprimées de : 2017-05-12_3_6C-80per_3_6C_CH2.csv\n",
      "Colonnes supprimées de : 2017-05-12_3_6C-80per_3_6C_CH3.csv\n",
      "Colonnes supprimées de : 2017-05-12_4C-80per_4C_CH5.csv\n",
      "Colonnes supprimées de : 2017-05-12_4C-80per_4C_CH6.csv\n",
      "Colonnes supprimées de : 2017-05-12_4_4C-80per_4_4C_CH7.csv\n",
      "Colonnes supprimées de : 2017-05-12_4_8C-80per_4_8C_CH10.csv\n",
      "Colonnes supprimées de : 2017-05-12_4_8C-80per_4_8C_CH9.csv\n",
      "Colonnes supprimées de : 2017-05-12_5_4C-80per_5_4C_CH11.csv\n",
      "Colonnes supprimées de : 2017-05-12_5_4C-80per_5_4C_CH12.csv\n",
      "Colonnes supprimées de : 2017-06-30_4_8C-80per_4_8C_CH4.csv\n",
      "Colonnes supprimées de : 2017-06-30_4_8C-80per_4_8C_CH7.csv\n",
      "Colonnes supprimées de : 2017-06-30_4_8C-80per_4_8C_CH8.csv\n",
      "Colonnes supprimées de : 2018-04-12_4_8C_80per_4_8C_newstructure_CH1.csv\n",
      "Colonnes supprimées de : 2018-04-12_4_8C_80per_4_8C_newstructure_CH17.csv\n",
      "Colonnes supprimées de : 2018-04-12_4_8C_80per_4_8C_newstructure_CH25.csv\n",
      "Colonnes supprimées de : 2018-04-12_4_8C_80per_4_8C_newstructure_CH9.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def remove_columns_from_csv(directory, columns_to_remove):\n",
    "    # Lister tous les fichiers CSV dans le répertoire donné\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "    # Parcourir chaque fichier CSV\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(directory, csv_file)\n",
    "        \n",
    "        # Lire le fichier CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Vérifier et supprimer les colonnes spécifiques si elles existent\n",
    "        df.drop(columns=[col for col in columns_to_remove if col in df.columns], inplace=True)\n",
    "        \n",
    "        # Enregistrer le fichier CSV modifié (remplace le fichier original)\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Colonnes supprimées de : {csv_file}\")\n",
    "\n",
    "directory = r'C:\\\\Users\\\\pc\\\\Desktop\\\\data\\\\all batchs'\n",
    "\n",
    "\n",
    "columns_to_remove = ['Test_Time', 'DateTime', 'Step_Time', 'Step_Index', 'Aux_Voltage','Data_Point','Charge_Capacity']\n",
    "\n",
    "\n",
    "remove_columns_from_csv(directory, columns_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lignes filtrées dans : 2017-05-12_3_6C-80per_3_6C_CH1.csv\n",
      "lignes filtrées dans : 2017-05-12_3_6C-80per_3_6C_CH2.csv\n",
      "lignes filtrées dans : 2017-05-12_3_6C-80per_3_6C_CH3.csv\n",
      "lignes filtrées dans : 2017-05-12_4C-80per_4C_CH5.csv\n",
      "lignes filtrées dans : 2017-05-12_4C-80per_4C_CH6.csv\n",
      "lignes filtrées dans : 2017-05-12_4_4C-80per_4_4C_CH7.csv\n",
      "lignes filtrées dans : 2017-05-12_4_8C-80per_4_8C_CH10.csv\n",
      "lignes filtrées dans : 2017-05-12_4_8C-80per_4_8C_CH9.csv\n",
      "lignes filtrées dans : 2017-05-12_5_4C-80per_5_4C_CH11.csv\n",
      "lignes filtrées dans : 2017-05-12_5_4C-80per_5_4C_CH12.csv\n",
      "lignes filtrées dans : 2017-06-30_4_8C-80per_4_8C_CH4.csv\n",
      "lignes filtrées dans : 2017-06-30_4_8C-80per_4_8C_CH7.csv\n",
      "lignes filtrées dans : 2017-06-30_4_8C-80per_4_8C_CH8.csv\n",
      "lignes filtrées dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH1.csv\n",
      "lignes filtrées dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH17.csv\n",
      "lignes filtrées dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH25.csv\n",
      "lignes filtrées dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH9.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def remove_rows(directory):\n",
    "    # Lister tous les fichiers CSV dans le répertoire donné \n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv') ]\n",
    "\n",
    "    # Parcourir chaque fichier CSV\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(directory, csv_file)\n",
    "        \n",
    "        # Lire le fichier CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "      \n",
    "        \n",
    "        # Supprimer les lignes où Cycle_Index est égal à 0\n",
    "        df = df[df['Cycle_Index'] != 0]\n",
    "        \n",
    "        # Enregistrer le fichier CSV modifié (remplace le fichier original)\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"lignes filtrées dans : {csv_file}\")\n",
    "\n",
    "# Répertoire contenant les fichiers CSV\n",
    "directory = r'C:\\Users\\pc\\Desktop\\data\\all batchs'  # Remplacez par votre chemin\n",
    "\n",
    "\n",
    "# Appel de la fonction\n",
    "remove_rows(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonne 'Cycle_Index' décrémentée dans : 2017-05-12_3_6C-80per_3_6C_CH1.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2017-05-12_3_6C-80per_3_6C_CH2.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2017-05-12_3_6C-80per_3_6C_CH3.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2017-05-12_4C-80per_4C_CH5.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2017-05-12_4C-80per_4C_CH6.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2017-05-12_4_4C-80per_4_4C_CH7.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2017-05-12_4_8C-80per_4_8C_CH10.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2017-05-12_4_8C-80per_4_8C_CH9.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2017-05-12_5_4C-80per_5_4C_CH11.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2017-05-12_5_4C-80per_5_4C_CH12.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2017-06-30_4_8C-80per_4_8C_CH4.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2017-06-30_4_8C-80per_4_8C_CH7.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2017-06-30_4_8C-80per_4_8C_CH8.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH1.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH17.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH25.csv\n",
      "Colonne 'Cycle_Index' décrémentée dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH9.csv\n"
     ]
    }
   ],
   "source": [
    "csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "# Parcourir chaque fichier CSV\n",
    "for csv_file in csv_files:\n",
    "    # Construire le chemin complet du fichier\n",
    "    file_path = os.path.join(directory, csv_file)\n",
    "    \n",
    "    # Lire le fichier CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    \n",
    "    if 'Cycle_Index' in df.columns:\n",
    "        df['Cycle_Index'] = df['Cycle_Index'] - 1\n",
    "        \n",
    "        # Enregistrer le fichier CSV modifié (remplace le fichier original)\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Colonne 'Cycle_Index' décrémentée dans : {csv_file}\")\n",
    "    else:\n",
    "        print(f\"Colonne 'Cycle_Index' non trouvée dans : {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlever le derniers cycle des donnee de 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lignes filtrées dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH1.csv\n",
      "lignes filtrées dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH17.csv\n",
      "lignes filtrées dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH25.csv\n",
      "lignes filtrées dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH9.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def remove_rows(directory):\n",
    "    # Lister tous les fichiers CSV dans le répertoire donné \n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv') and f.startswith('2018')]\n",
    "\n",
    "    # Parcourir chaque fichier CSV\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(directory, csv_file)\n",
    "        \n",
    "        # Lire le fichier CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "        derniere_valeur = df['Cycle_Index'].iloc[-1]\n",
    "\n",
    "        # Supprimer les lignes où Cycle_Index est égal à 0\n",
    "        df = df[df['Cycle_Index'] != derniere_valeur]\n",
    "        \n",
    "        # Enregistrer le fichier CSV modifié (remplace le fichier original)\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"lignes filtrées dans : {csv_file}\")\n",
    "\n",
    "# Répertoire contenant les fichiers CSV\n",
    "directory = r'C:\\Users\\pc\\Desktop\\data\\all batchs'  # Remplacez par votre chemin\n",
    "# Appel de la fonction\n",
    "remove_rows(directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traitement des exeption restante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "file_path='C:\\\\Users\\\\pc\\\\Desktop\\\\data\\\\all batchs\\\\2017-05-12_3_6C-80per_3_6C_CH1.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Suppression du cycle 11\n",
    "df = df[df['Cycle_Index'] != 11]\n",
    "\n",
    "# Décrémenter tous les Cycle_Index supérieurs à 11\n",
    "df.loc[df['Cycle_Index'] > 11, 'Cycle_Index'] -= 1\n",
    "df.to_csv(file_path, index=False)\n",
    "df = df[(df['Cycle_Index'] != 1189)&(df['Cycle_Index']!=1188)]\n",
    "\n",
    "# Décrémenter tous les Cycle_Index supérieurs à 11\n",
    "df.loc[df['Cycle_Index'] > 1189, 'Cycle_Index'] -= 2\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "file_path='C:\\\\Users\\\\pc\\\\Desktop\\\\data\\\\all batchs\\\\2017-05-12_3_6C-80per_3_6C_CH2.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df = df[(df['Cycle_Index'] != 1179)&(df['Cycle_Index']!=1178)]\n",
    "\n",
    "# Décrémenter tous les Cycle_Index supérieurs à 11\n",
    "df.loc[df['Cycle_Index'] > 1179, 'Cycle_Index'] -= 2\n",
    "df.to_csv(file_path, index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "file_path='C:\\\\Users\\\\pc\\\\Desktop\\\\data\\\\all batchs\\\\2017-05-12_3_6C-80per_3_6C_CH3.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df = df[(df['Cycle_Index'] != 1177)&(df['Cycle_Index']!=1176)]\n",
    "\n",
    "# Décrémenter tous les Cycle_Index supérieurs à 11\n",
    "df.loc[df['Cycle_Index'] > 1177, 'Cycle_Index'] -= 2\n",
    "df.to_csv(file_path, index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "file_path='C:\\\\Users\\\\pc\\\\Desktop\\\\data\\\\all batchs\\\\2017-05-12_4_4C-80per_4_4C_CH7.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df = df[(df['Cycle_Index'] != 908)]\n",
    "\n",
    "# Décrémenter tous les Cycle_Index supérieurs à 11\n",
    "df.loc[df['Cycle_Index'] >908, 'Cycle_Index'] -= 1\n",
    "df.to_csv(file_path, index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle_Index où le maximum de Discharge Capacity est inférieur à 0.95 :\n",
      "Index([908.0], dtype='float64', name='Cycle_Index')\n"
     ]
    }
   ],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "\n",
    "# Chemin du fichier CSV\n",
    "file_path = 'C:\\\\Users\\\\pc\\\\Desktop\\\\data\\\\all batchs\\\\2017-05-12_4_4C-80per_4_4C_CH7.csv'\n",
    "\n",
    "# Lire le fichier CSV\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filtrer les lignes où 'Cycle_Index' est inférieur à 1500\n",
    "filtered_df = df[df['Cycle_Index'] < 1000]\n",
    "\n",
    "# Calculer le maximum de 'Discharge_Capacity' par 'Cycle_Index' dans les cycles filtrés\n",
    "max_discharge_capacity = filtered_df.groupby('Cycle_Index')['Discharge_Capacity'].max()\n",
    "\n",
    "# Filtrer les 'Cycle_Index' où le maximum de 'Discharge_Capacity' est inférieur à 0.95\n",
    "cycles_below_095 = max_discharge_capacity[max_discharge_capacity < 0.925].index\n",
    "\n",
    "# Afficher les 'Cycle_Index' correspondants\n",
    "print(\"Cycle_Index où le maximum de Discharge Capacity est inférieur à 0.95 :\")\n",
    "print(cycles_below_095)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supprimer les cycles ou max discharge capacity est inférieur a 0.880000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycles filtrés dans : 2017-05-12_3_6C-80per_3_6C_CH1.csv\n",
      "Cycles filtrés dans : 2017-05-12_3_6C-80per_3_6C_CH2.csv\n",
      "Cycles filtrés dans : 2017-05-12_3_6C-80per_3_6C_CH3.csv\n",
      "Cycles filtrés dans : 2017-05-12_4C-80per_4C_CH5.csv\n",
      "Cycles filtrés dans : 2017-05-12_4C-80per_4C_CH6.csv\n",
      "Cycles filtrés dans : 2017-05-12_4_4C-80per_4_4C_CH7.csv\n",
      "Cycles filtrés dans : 2017-05-12_4_8C-80per_4_8C_CH10.csv\n",
      "Cycles filtrés dans : 2017-05-12_4_8C-80per_4_8C_CH9.csv\n",
      "Cycles filtrés dans : 2017-05-12_5_4C-80per_5_4C_CH11.csv\n",
      "Cycles filtrés dans : 2017-05-12_5_4C-80per_5_4C_CH12.csv\n",
      "Cycles filtrés dans : 2017-06-30_4_8C-80per_4_8C_CH4.csv\n",
      "Cycles filtrés dans : 2017-06-30_4_8C-80per_4_8C_CH7.csv\n",
      "Cycles filtrés dans : 2017-06-30_4_8C-80per_4_8C_CH8.csv\n",
      "Cycles filtrés dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH1.csv\n",
      "Cycles filtrés dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH17.csv\n",
      "Cycles filtrés dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH25.csv\n",
      "Cycles filtrés dans : 2018-04-12_4_8C_80per_4_8C_newstructure_CH9.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def remove_cycles_with_low_discharge(directory, threshold=0.880000):\n",
    "    # Lister tous les fichiers CSV dans le répertoire donné\n",
    "    csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "    # Parcourir chaque fichier CSV\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(directory, csv_file)\n",
    "\n",
    "        # Lire le fichier CSV\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Calculer le maximum de Discharge Capacity par Cycle_Index\n",
    "        max_discharge_capacity = df.groupby('Cycle_Index')['Discharge_Capacity'].max()\n",
    "\n",
    "        # Filtrer les Cycle_Index où le maximum de Discharge Capacity est supérieur ou égal au seuil\n",
    "        valid_cycles = max_discharge_capacity[max_discharge_capacity >= threshold].index\n",
    "\n",
    "        # Garder seulement les lignes dont le Cycle_Index est dans les cycles valides\n",
    "        df_filtered = df[df['Cycle_Index'].isin(valid_cycles)]\n",
    "\n",
    "        # Enregistrer le fichier CSV modifié (remplace le fichier original)\n",
    "        df_filtered.to_csv(file_path, index=False)\n",
    "        print(f\"Cycles filtrés dans : {csv_file}\")\n",
    "\n",
    "# Répertoire contenant les fichiers CSV\n",
    "directory = r'C:\\Users\\pc\\Desktop\\data\\all batchs'  # Remplacez par votre chemin\n",
    "\n",
    "# Appel de la fonction\n",
    "remove_cycles_with_low_discharge(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import os\\nimport pandas as pd\\n\\n# Chemin vers le répertoire contenant les fichiers CSV\\ndirectory = r'C:\\\\Users\\\\pc\\\\Desktop\\\\data\\\\all batchs'\\n\\n# Lister tous les fichiers CSV dans le répertoire\\ncsv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import pandas as pd\n",
    "\n",
    "# Chemin vers le répertoire contenant les fichiers CSV\n",
    "directory = r'C:\\\\Users\\\\pc\\\\Desktop\\\\data\\\\all batchs'\n",
    "\n",
    "# Lister tous les fichiers CSV dans le répertoire\n",
    "csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Ouvrir un fichier texte pour écrire les résultats\\nwith open(\\'output_data_types.txt\\', \\'w\\') as f:\\n    for file in csv_files:\\n        file_path = os.path.join(directory, file)\\n        f.write(f\"\\n\\nFichier: {file}\\n\")\\n        \\n        # Charger le fichier CSV dans un DataFrame Pandas\\n        df = pd.read_csv(file_path)\\n        \\n        \\n        # Écrire les types de colonnes dans le fichier texte\\n        f.write(\"Types de données :\\n\")\\n        f.write(df.dtypes.to_string())\\n        f.write(\"\\n\\n\")\\n\\n        # Vérifier s\\'il y a des valeurs manquantes dans chaque colonne\\n        missing_values = df.isnull().sum()\\n        missing_values_columns = missing_values[missing_values > 0]\\n\\n        # Écrire les colonnes avec des valeurs manquantes dans le fichier texte\\n        if not missing_values_columns.empty:\\n            f.write(\"Colonnes avec des valeurs manquantes :\\n\")\\n            f.write(missing_values_columns.to_string())\\n        else:\\n            f.write(\"Aucune valeur manquante dans les colonnes.\\n\")'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Ouvrir un fichier texte pour écrire les résultats\n",
    "with open('output_data_types.txt', 'w') as f:\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        f.write(f\"\\n\\nFichier: {file}\\n\")\n",
    "        \n",
    "        # Charger le fichier CSV dans un DataFrame Pandas\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        \n",
    "        # Écrire les types de colonnes dans le fichier texte\n",
    "        f.write(\"Types de données :\\n\")\n",
    "        f.write(df.dtypes.to_string())\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        # Vérifier s'il y a des valeurs manquantes dans chaque colonne\n",
    "        missing_values = df.isnull().sum()\n",
    "        missing_values_columns = missing_values[missing_values > 0]\n",
    "\n",
    "        # Écrire les colonnes avec des valeurs manquantes dans le fichier texte\n",
    "        if not missing_values_columns.empty:\n",
    "            f.write(\"Colonnes avec des valeurs manquantes :\\n\")\n",
    "            f.write(missing_values_columns.to_string())\n",
    "        else:\n",
    "            f.write(\"Aucune valeur manquante dans les colonnes.\\n\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#remplacer les valeurs manquantes\\nfor csv_file in csv_files:\\n    # Construire le chemin complet du fichier\\n    file_path = os.path.join(directory, csv_file)\\n    \\n    # Lire le fichier CSV\\n    df = pd.read_csv(file_path)\\n    # Remplir les valeurs manquantes\\n    df=df.bfill()\\n    df.to_csv(file_path, index=False)\\n       \\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#remplacer les valeurs manquantes\n",
    "for csv_file in csv_files:\n",
    "    # Construire le chemin complet du fichier\n",
    "    file_path = os.path.join(directory, csv_file)\n",
    "    \n",
    "    # Lire le fichier CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Remplir les valeurs manquantes\n",
    "    df=df.bfill()\n",
    "    df.to_csv(file_path, index=False)\n",
    "       \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfor csv_file in csv_files:\\n    file_path = os.path.join(directory, csv_file)\\n    \\n    # Vérifie si le fichier existe\\n    if not os.path.isfile(file_path):\\n        print(f\"Le fichier {file_path} n\\'existe pas.\")\\n        continue\\n    \\n    # Lire le fichier CSV\\n    try:\\n        df = pd.read_csv(file_path)\\n    except Exception as e:\\n        print(f\"Erreur lors de la lecture du fichier {file_path}: {e}\")\\n        continue\\n    \\n    # Détection des outliers\\n    outlier_report = []\\n    \\n    for col in df.select_dtypes(include=[\\'float64\\', \\'int64\\']).columns:\\n        Q1 = df[col].quantile(0.25)\\n        Q3 = df[col].quantile(0.75)\\n        IQR = Q3 - Q1\\n\\n        lower_bound = Q1 - 1.5 * IQR\\n        upper_bound = Q3 + 1.5 * IQR\\n\\n        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\\n        if not outliers.empty:\\n            outlier_report.append(f\"Colonne \\'{col}\\':\")\\n            outlier_report.append(f\"Index des lignes avec outliers : {list(outliers.index)}\")\\n            outlier_report.append(\"\")  # Ligne vide pour séparer les colonnes\\n    \\n    # Si des outliers sont trouvés, enregistrer dans un fichier à part\\n    if outlier_report:\\n        output_file = os.path.join(directory, f\"{os.path.splitext(csv_file)[0]}_outliers.txt\")\\n        try:\\n            with open(output_file, \\'w\\') as f_out:\\n                f_out.write(f\"Fichier: {csv_file}\\n\")\\n                f_out.write(\"\\n\".join(outlier_report))\\n            print(f\"Rapport des outliers pour {csv_file} généré dans {output_file}.\")\\n        except Exception as e:\\n            print(f\"Erreur lors de l\\'écriture du fichier {output_file}: {e}\")\\n    else:\\n        print(f\"Aucun outlier détecté dans le fichier {csv_file}.\")\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory, csv_file)\n",
    "    \n",
    "    # Vérifie si le fichier existe\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"Le fichier {file_path} n'existe pas.\")\n",
    "        continue\n",
    "    \n",
    "    # Lire le fichier CSV\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier {file_path}: {e}\")\n",
    "        continue\n",
    "    \n",
    "    # Détection des outliers\n",
    "    outlier_report = []\n",
    "    \n",
    "    for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        if not outliers.empty:\n",
    "            outlier_report.append(f\"Colonne '{col}':\")\n",
    "            outlier_report.append(f\"Index des lignes avec outliers : {list(outliers.index)}\")\n",
    "            outlier_report.append(\"\")  # Ligne vide pour séparer les colonnes\n",
    "    \n",
    "    # Si des outliers sont trouvés, enregistrer dans un fichier à part\n",
    "    if outlier_report:\n",
    "        output_file = os.path.join(directory, f\"{os.path.splitext(csv_file)[0]}_outliers.txt\")\n",
    "        try:\n",
    "            with open(output_file, 'w') as f_out:\n",
    "                f_out.write(f\"Fichier: {csv_file}\\n\")\n",
    "                f_out.write(\"\\n\".join(outlier_report))\n",
    "            print(f\"Rapport des outliers pour {csv_file} généré dans {output_file}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'écriture du fichier {output_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"Aucun outlier détecté dans le fichier {csv_file}.\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
